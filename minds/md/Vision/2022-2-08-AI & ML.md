---
ayout:     post
title:      "AI & ML - UoB 06-36404"
subtitle:   " \"Basic AI algorithms!\""
date:       2022-02-08 12:00:00
author:     "Calvchen"
header-img: "img/post-bg-infinity.jpg"
catalog: true
tags:
    - Math
    - AI
    - ML
    - Course


---

> “Machine Learning, Deep Learning and Data Science, etc. are all in the now same nexus with the rapid evolution of computer power. ..”

# AI & ML

AI and ML are getting more and more popular recently, the boundries between AI, ML and traditional optimization algorithms, even data sciense are becoming vague.

## What will be covered?

- [Evaluation](#Model Evaluation)
- Basic Theory (Will jump to Wiki)
  - [Kernel machines](https://en.wikipedia.org/wiki/Kernel_machines), [Bias–variance tradeoff](https://en.wikipedia.org/wiki/Bias–variance_tradeoff), [Computational learning theory](https://en.wikipedia.org/wiki/Computational_learning_theory), [Empirical risk minimization](https://en.wikipedia.org/wiki/Empirical_risk_minimization), [Occam learning](https://en.wikipedia.org/wiki/Occam_learning), [PAC learning](https://en.wikipedia.org/wiki/Probably_approximately_correct_learning), [Statistical learning](https://en.wikipedia.org/wiki/Statistical_learning_theory), [VC theory](https://en.wikipedia.org/wiki/Vapnik–Chervonenkis_theory)

- [Clustering](#Clustering)
  - [Hierarchical Clustering](#Hierarchical Clustering)
  - [K-means](#K-means), Mean shift, KNN,  [GMM](#gmms)
  - Density-based clustering, e.g. [DBScan](#DBScan) 
  - Mixture density based clusterin
  - Fuzzy theory based, graph theory based, grid based, etc.
- Supervised Learning
  - Naive Bayes
  - RVM, SVM
  - Tree Model
    - Decision Tree, XGBoost, LGBboost

- Dimension Reduction
  - t-SNE, PCA, factor analysis, PGD
- Loss Function
  - Softmax
  - Triplet
  - ArcFace
- Ensemble
  - Bagging, Boosting, Random Forest

- Graphical Models (Sturctured Prediction)
  - Bayes Net, Conditional Random Field, Hidden Markov

- Reinforcement Learning
  - Q-Learning, SARSA, Temporal Difference (TD)


**AI Taxonomy**

![image-20220211093945234](https://ik.imagekit.io/haochen/Typora/image-20220211093945234.png)

## Model Evaluation

§ Accuracy (1-Error) 

§ ROC, AUC 

§ Precision, Recall

§ F-measure, G-mean 

§ (Cross) Entropy 

§ Likelihood 

§ Squared Error/MSE 

§ R2, et cetera.

### Distance Measurement

🌍 **Euclidean:**
$$
D(\vec x,\vec y)=\sqrt{(x_1-y_1)^2+\cdots+(x_n-y_n)^2}
$$


🌍 **Manhattan:**
$$
D(\vec x,\vec y)=\sum_{i=1}^n \left| x_i - y_i \right|
$$


🌍 **Chebyshev:**
$$
D(\vec x,\vec y)=\max_{i}( \left| x_i - y_i \right|)
$$


🌍 **Minkowski:**
$$
D(\vec x,\vec y)= \left( \sum_{i=1}^n  |x_i - y_i|^b \right)^{1/b}
$$





## Clustering

![../../_images/plot_cluster_comparison_0011.png](https://scikit-learn.org/0.15/_images/plot_cluster_comparison_0011.png)

### Hierarchical Clustering

A top-down tree method, agglomerative to devisive.

![image-20220211095149931](https://ik.imagekit.io/haochen/Typora/image-20220211095149931.png)

```
from sklearn.cluster import AgglomerativeClustering #导入sklearn的层次聚类函数
model = AgglomerativeClustering(n_clusters = k, linkage = 'ward')
model.fit(data) #训练模型

#详细输出原始数据及其类别
r = pd.concat([data, pd.Series(model.labels_, index = data.index)], axis = 1)  #详细输出每个样本对应的类别
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import linkage,dendrogram

#这里使用scipy的层次聚类函数
Z = linkage(data, method = 'ward', metric = 'euclidean') #谱系聚类图
P = dendrogram(Z, 0) #画谱系聚类图
plt.show()

```

**Group Merging Distance Metric:**

- Single Linkage: the similarity of the **closest** pair.
- Complete Linkage: the similarity of the **furthest** pair.
- Group Average: the **average** similarity of all pairs, more widely used against noise (robust).

**Agglomerative Grouping Loop:**

1. Place each data point into its own singleton group 
2. Repeat: iteratively merge the two **closest groups** according to  
3. Until: all the data are merged into a single cluster

**Strengths:**

- Provides deterministic results, and can create arbitrary shapes
- Do not need to specify the number of clusters $K$

**Weakness:**

- Does not scale up for large datasets, the time complexity is at least $\text O(n^2)$
- Caveats: can be applied to data even not suitable for hierarchical structure.



### K-means

**Goal**: minimise the within-cluster variances of every centroid-based cluster.

**Loop:**

1. Set up parameter $K$
2. Randomly assign $K$ centroids
3. Calculate the distance and adjust the **means (centroids)**
4. Reassign data points, and go back to **(3)** until no cluster changes

**Disadvantage:**

- Intialization has an enormous impact on results (multiple restarts are often  necessary)
- $K$ is sensitive

#### Algorithm Description

**Initialization**

- Data $X \in\mathbb R^{N\times d}$ and given cluster number $k$.
- Choose initial cluster means $m\in\mathbb R^{k\times d}$ (same dimension as data).

**Repeat (In the Loop)**

1. Assign each data point to its closest mean (Euclidean distance)

$$
z_{n}=\arg \min _{i \in\{1, \ldots, k\}} \mathbb d\left(x_{n}, m_{i}\right) \\\Longrightarrow \mathbb d(p, q)=\sqrt{\sum^d_{i=1}(p_i-q_i)^2} .
$$


2. Compute each cluster mean to be the **coordinate-wise average** over data points assigned to that cluster,

$$
m_{k}=\frac{1}{N_{k}} \sum_{\left\{n: z_{n}=k\right\}} x_{n}
$$
### KNN

k-Nearest Neighbours

- **instance-** or **memory-based learning** , like table lookup

- Problem: if the value does not exist, then a default value is returned

major vote

**For Regression Problems**: average or median of the values of the k nearest neighbours

Different Scale in coordinate may result in problem when calculating the distance.

![image-20220326205842112](https://ik.imagekit.io/haochen/Typora/image-20220326205842112.png)



### GMMs

**Gaussian Mixture Models** is a approximated density estimator, assuming data was generated by a set of Gaussian distributions (Components).

We have $X\in \mathbb R^{N\times d},\ \mu\in \mathbb R^{1\times d},\sigma\in \mathbb R^{d\times d}$.

A GMM represents a distribution as:
$$
p(x)=\sum_{k=1}^k\pi_k\ \mathcal N(x|\mu_k, \sigma^2)
$$


where $\pi_i=p(z_i)$ is the probablity of choosing the $i^{th}$ distribution.

**Goal:** Find the parameters vector $(\pi, \mu, \sigma)$ of Guissian distributions based on **EM** Algorithm.

#### Expectation Maximization

Let’s say we have latend variable $z$, which is unobservable. Our goal is to maximise the **marginal likelihood** of $X$ given our parameters (denoted by the vector $\theta$).  

Essentially we can find the marginal distribution as the joint of $X$ and $Z$ and sum over all $Z$’s (sum rule of probability), and we want to maximize the log-likelyhood (对数似然):


$$
L =\ln p(X|\theta) = \ln \left\{ \sum_{k=1}^K p(X,z_k|\theta) \right\}
$$


The above equation often result in complicated function that is hard to maximize.

And in the notation that we have introduced $p(x)$ is just:


$$
\begin{array}{l}
p(X)=\prod^N_{i=1}p(x_i)\\
\log p(X) = \log \left(\sum_{i=1}^N p(x_i)\right)  \\
p(x_i) = \sum_{k}^K p(x_i|z_k)p(z_k)
\end{array}
$$



Then, what we can do in this case is to use [Jensens Inequality](https://en.wikipedia.org/wiki/Jensen's_inequality) to construct a **lower bound** function which is much easier to optimise. If we optimise this by minimising the [KL divergence](https://www.coursera.org/learn/bayesian-methods-in-machine-learning/lecture/jyYT0/e-step-details) (gap) between the two distributions we can approximate the original function. 

**E-step:**

In the E-step we want to estimate the **posterior distribution** (后验概率) of our latent variables 𝛾 conditional on our parameters $(\pi, \mu, \sigma)$ of our Gaussians.


$$
p(x) = \sum_{k=1}^K \pi_k\ \mathcal N(x|\mu_k, \sigma_k^2)
$$



We want to calculate the posterior distribution $\gamma (z_{nk})$  of the **responsibilities** that $k^{th}$ Gaussian has for $n^{th}$ data point using the formula below. 

Responsibility is the other name of $P(Z|x)$ here.
$$
\gamma(z_{nk}) = \frac{\pi_k N(x_n|\mu_k , \sigma^2_k)}{\sum_{j=1}^K  \pi_jN(x_n|\mu_j , \sigma^2_j)}
$$



The above of the fractional item is the possibility of  chosen Gaussian, the below item is the overall sum of possibilities accross all Gaussian.

The simplified derivation process:


$$
\begin{array}{l}

\gamma_{nk} &= p(z_k|x_n)=\frac{p(z_k\cap x_n)}{p(x_n)} \\
&=\frac{p(x_n|z_k)p(z_k)}{\sum_{j=1}^K p(x_n|z_j)p(z_j)}

\end{array}
$$



Given:

$$
p(z_c) = \pi_c\\
p(x|z_k) = N_k(x)
$$



**M-step:**

M-step use $\gamma$ to maximize the **likelyhood function** of GMMs respect to parameters $\theta$.


$$
\begin{array}{l}
m_{k}=\sum_{i=1}^{N} \gamma_{i k} \\
\pi_{k}=\frac{m_{k}}{N}\\
\mu_{k}=\frac{1}{m_{k}} \sum_{i=1}^{N} \gamma_{i k} x_{i}\\
\sigma^2_{k}=\frac{1}{m_{k}} \sum_{i=1}^{N} \gamma_{ik}\left(x_{i}-\mu_{k}\right)^{T}\left(x_{i}-\mu_{k}\right)
\end{array}
$$



Update the parameters $\theta$ by calculating **maximum likelihood estimators (MLE)** $L_k$ for each cluster $k$.

Repeat the E-M steps until log-likelihood value $\ln p(X|\theta)$ converges.

**Convergence:**

贝叶斯学派视角下的一类点估计法称为贝叶斯估计，常用的贝叶斯估计有最大后验估计（Maximum A Posteriori Estimation，简称MAP）、后验中位数估计和后验期望值估计这3种参数估计, 之后会补充证明。

**Recap:**

- k-means can be used to search a good starting point



### DBScan

Acronym for ***Density-based** spatial clustering of applications with noise*.

- A cluster is defined as a maximal set of **density connected points.** 
- Discover clusters of arbitrary shape.

<img src="https://ik.imagekit.io/haochen/Typora/Kernel_Machine.svg" alt="img" style="zoom:1%;" />

The DBSCAN algorithm basically requires 2 parameters:

- **eps $\epsilon$:**  specifies how close points should be to each other to be considered a part of a cluster.
  - if too small, data may not be well clustered.
  - if too high, cluster will merge the majority of objects.

- **minPoints: **the minimum number of points to form a dense region.
  - As a general rule, a minimum minPoints can be derived from a number of dimensions $(D)$ in the data set, as minPoints $\geq D+1$. 
  - Larger values are usually better for data sets with noise and will form more significant clusters. The larger the data set, the larger the minPoints value that should be chosen.

**Types of Points:**
$$
\begin{array}{|l|l|}
\hline \text { core } & \begin{array}{l}
\text { The point has at least minPts } \\
\text { number of points within }\epsilon
\end{array} \\
\hline \text { border } & \begin{array}{l}
\text { The point has fewer than minPts } \\
\text { within }\epsilon \text{, but is in the } \\
\text { neighbourhood (i.e. circle) of a core } \\
\text { point. }
\end{array} \\
\hline \text { noise } & \begin{array}{l}
\text { Any point that is not a core point or a } \\
\text { border point. }
\end{array} \\
\hline
\end{array}
$$
**Loop:**

1. Extract new object $p$ from data set as Start Point, all its neighbors which distance between them is lower than $\varepsilon$, the number of neighbors less than minPoints called **border** ,otherwise called **core**.
2. Traverse the whole dataset, iteratively find all **cores’** neighbors **(density-reachable)**, assigned label **core** or **border**.
3. Go back to **(2)** until all the objects are processed.
4. Eliminate noisy points.
5. Assign each non-core point to a nearby cluster if the cluster is an $\varepsilon$ (eps) neighbor, otherwise assign it to noise.

**Or overall method：**

1. Find the points in the ε (eps) neighborhood of every point, and identify the core points with more than minPts neighbors.
2. Find the [connected components](https://en.wikipedia.org/wiki/Connected_component_(graph_theory)) of ***core*** points on the neighbor graph, ignoring all non-core points.
3. Assign each non-core point to a nearby cluster if the cluster is an **ε (eps)** neighbor, otherwise assign it to noise.

**Advantage:** 

- Arbitrary shape of clustering
- Don’t need to specify $k$ of clusters
- Resist to noise.

**Disadvantage:** 

- Cannot find non-linearly separable clusters (an advantage over K-means  and GMM)
- Not entirely deterministic: border points that are reachable from more than one point.



## Supervised Method

Assume we have the $i^{th}$ feature of the $n^{th}$ example  is $x^{(n)}_i$, the related label is $y^{(n)}$.


$$
\mathbf x = \left(x^{(1)}, y^{(1)}\right),\left(x^{(2)}, y^{(2)}\right), \ldots,\left(x^{(n)}, y^{(n)}\right)
$$

### Linear Regression

$$
\mathbf y = f(\mathbf x;\mathbf w, b)=w_1x + b_1
$$

**Pictorial loss**

By minimising the cost function
$$
\mathrm{g}\left(w_{1}, b\right)=\frac{1}{N} \sum_{n=1}^{N}\left(\left(w_{1} x^{(n)}+b\right)-y^{(n)}\right)^{2}
$$
<img src="https://ik.imagekit.io/haochen/Typora/image-20220224153451692.png" alt="image-20220224153451692" style="zoom:33%;" />

**Univariate linear regression** 

We want to find values for weight than the cost is minimum

### Logistic Regression

a linear classification model assuming $X \sim Ber(p)$, could be a line, plane or hyperplane (more than 3-dimension).

<img src="https://upload.wikimedia.org/wikipedia/commons/3/33/Sigmoid_function_01.png" alt="img" style="zoom:33%;" />
$$
\sigma(x;w) = \frac{1}{1+e^{-x}}
$$
What if we predict output exactly equal to the threshold or lie on the decision boundary? very slim possible.

Other similar **S-functions** (slope normalised to 1 and shifted to the origin)

![img](https://upload.wikimedia.org/wikipedia/commons/thumb/6/6f/Gjl-t%28x%29.svg/1024px-Gjl-t%28x%29.svg.png)

### **Logistic Cost Function**

![image-20220304125954699](https://ik.imagekit.io/haochen/Typora/image-20220304125954699.png)

is **convex.**

![image-20220304130210576](https://ik.imagekit.io/haochen/Typora/image-20220304130210576.png)



![image-20220304130321976](https://ik.imagekit.io/haochen/Typora/image-20220304130321976.png)

plug-in

![image-20220304130331804](https://ik.imagekit.io/haochen/Typora/image-20220304130331804.png)

![image-20220304130350371](https://ik.imagekit.io/haochen/Typora/image-20220304130350371.png)

Multi class logistic regression: uses a multi valued version of sigmoid?

## Optimization





### Neural Network

CE can be used for regres

![image-20220304151231469](https://ik.imagekit.io/haochen/Typora/image-20220304151231469.png)





### Gradient Descent

minimize the cost function $g(\mathbf w)$,
$$
\mathbf w:=\mathbf w - \alpha\nabla g(\mathbf w)
$$
<img src="https://ik.imagekit.io/haochen/Typora/image-20220224153947160.png" alt="image-20220224153947160" style="zoom:33%;" />

![image-20220224154026158](https://ik.imagekit.io/haochen/Typora/image-20220224154026158.png)

Solve Univariate Linear Regression

![image-20220224154046003](https://ik.imagekit.io/haochen/Typora/image-20220224154046003.png)

Multivariate and Univariate (多变量和单变量)

![image-20220304124042979](https://ik.imagekit.io/haochen/Typora/image-20220304124042979.png)



## Loss Functions

In this part, we pre-define some universal factors for below loss functions.

$x_{i} \in \mathbb{R}^{d}$ denotes the deep features of the $i$-th training sample (belonging to the $y_{i}$-th class). 

$W_{j,k} \in \mathbb{R}^{d\times n}$ denotes the $j$-th column of the weight, and $b_{j} \in \mathbb{R}^{n}$ is the bias term. So logits denoted as $W_j^T x_i + b_j$.

$N$ and $n$ is the number of batchsize and class.

$d(\cdot)$ denotes the distance on the embedding space.

### Mean Square Error (MSE)

<img src="https://ik.imagekit.io/haochen/Typora/image-20220304123929505.png" alt="image-20220304123929505" style="zoom:33%;" />

### Softmax

The most widely used classification loss function, softmax loss, is presented as follows:
$$
L_{1}=-\frac{1}{N} \sum_{i=1}^{N} \log \frac{e^{W_{y_{i}}^{T} x_{i}+b_{y_{i}}}}{\sum_{j=1}^{n} e^{W_{j}^{T} x_{i}+b_{j}}}
$$


### Triplet

[PDF](https://arxiv.org/abs/1503.03832), [Code-tensorflow](https://github.com/omoindrot/tensorflow-triplet-loss)

**Why not just use softmax?**

Softmax train network with fixed $n$ that do not satisfy the need of face recognation.

**Algorithm**

Given three logits from ($a, p \in y_1,\ n \in y_2$) samples, we want to minimize the distance between two positive samples ($a, p$) and maximize the distance between two negative samples ($a, n$). 
$$
\mathcal L = max(d(a,p) - d(a,n) + margin, 0)
$$


which push $d(a,p)$ to 0 and $d(a,n)$ to be greater than $d(a, p) + margin$. 

##### Triplet mining

Based on the definition of the loss, there are three categories of triplets:

- **easy triplets:** triplets which have a loss of 0 , because $d(a, p)+$ margin $<d(a, n)$
- **hard triplets:** triplets where the negative is closer to the anchor than the positive, i.e. $d(a, n)<d(a, p)$
- **semi-hard triplets**: triplets where the negative is not closer to the anchor than the positive, but which still have positive loss:
$d(a, p)<d(a, n)<d(a, p)+$ margin

Each of these definitions depend on where the negative is, relatively to the anchor and positive.

<img src="https://omoindrot.github.io/assets/triplet_loss/triplets.png" alt="img" style="zoom:67%;" />

### **Naïve Bayes for Categorical Independent Variables**

Assumption: each input variable is **conditionally independent** of any other input variables given the output

<img src="https://ik.imagekit.io/haochen/Typora/image-20220326203133550.png" alt="image-20220326203133550" style="zoom:33%;" />



<img src="https://ik.imagekit.io/haochen/Typora/image-20220326203709868.png" alt="image-20220326203709868" style="zoom:50%;" />

？？why equal ？



**Naïve Bayes for Numerical Independent Variables**

Form distribution from observed data: 

<img src="https://ik.imagekit.io/haochen/Typora/image-20220326204219873.png" alt="image-20220326204219873" style="zoom:50%;" />



<img src="https://ik.imagekit.io/haochen/Typora/image-20220326204527412.png" alt="image-20220326204527412" style="zoom:25%;" />

<img src="https://ik.imagekit.io/haochen/Typora/image-20220326204635045.png" alt="image-20220326204635045" style="zoom:50%;" />

Pros and Cons of Naive Bayes

![image-20220326204717717](https://ik.imagekit.io/haochen/Typora/image-20220326204717717.png)



### Uninformed Search (Blind)

**asymptotic analysis**

How to measure the performance of the algorithms

![image-20220326200851590](https://ik.imagekit.io/haochen/Typora/image-20220326200851590.png)

Only consider the input size $n$.

<img src="https://ik.imagekit.io/haochen/Typora/image-20220326201003581.png" alt="image-20220326201003581" style="zoom:33%;" />

<img src="https://ik.imagekit.io/haochen/Typora/image-20220326201032936.png" alt="image-20220326201032936" style="zoom:30%;" />

Big O is bounded above, Big Theta is bounded below and Big Omega is bounded both of above and below.

<img src="https://ik.imagekit.io/haochen/Typora/image-20220326201059591.png" alt="image-20220326201059591" style="zoom:30%;" />

How to do it?

1. Abstract the input.
2. Abstract the 

<img src="https://ik.imagekit.io/haochen/Typora/image-20220326201627350.png" alt="image-20220326201627350" style="zoom:33%;" />



Search  from the initial state to the goal state

![image-20220326202422028](https://ik.imagekit.io/haochen/Typora/image-20220326202422028.png)

![image-20220326202522802](https://ik.imagekit.io/haochen/Typora/image-20220326202522802.png)





### Breadth-first Search (BFS)

• The root node is expanded first
 • Then, all the successors of the root node are expanded
 • Then, the successors of each of these nodes

- This is equivalent to expanding the shallowest unexpanded node in the frontier; simply use a **queue** (**FIFO**) for expansion

**Like brutal search?** # TODO

![image-20220326214610823](https://ik.imagekit.io/haochen/Typora/image-20220326214610823.png)

![image-20220326214737115](https://ik.imagekit.io/haochen/Typora/image-20220326214737115.png)

![image-20220326215049982](https://ik.imagekit.io/haochen/Typora/image-20220326215049982.png)

![image-20220326215243291](https://ik.imagekit.io/haochen/Typora/image-20220326215243291.png)



Time and space complexity: $O(b^d)$



### Depth-first Search (DFS)

m is the maximize depth of the tree

<img src="https://ik.imagekit.io/haochen/Typora/image-20220326215457267.png" alt="image-20220326215457267" style="zoom:33%;" />

<img src="https://ik.imagekit.io/haochen/Typora/image-20220326215507490.png" alt="image-20220326215507490" style="zoom:33%;" />

<img src="https://ik.imagekit.io/haochen/Typora/image-20220326215546206.png" alt="image-20220326215546206" style="zoom: 33%;" />

<img src="https://ik.imagekit.io/haochen/Typora/image-20220326215610231.png" alt="image-20220326215610231" style="zoom: 33%;" />

A variation of it: 

***DFS with less memory usage*** (saving space complexity): **remove the explored left tree**, pace complexity to 𝑂(𝑏𝑚)

***Depth-Limited Search***: set a limit $\mathcal l$, time complexity $O(b^l)$,

they can combined together.

<img src="https://ik.imagekit.io/haochen/Typora/image-20220326220127567.png" alt="image-20220326220127567" style="zoom:50%;" />



### Informed Search

#### Best-first Search

<img src="https://ik.imagekit.io/haochen/Typora/image-20220429095100996.png" alt="image-20220429095100996" style="zoom:50%;" />

Heuristic: easy to compute, not guarentee to work

![image-20220429095227123](https://ik.imagekit.io/haochen/Typora/image-20220429095227123.png)



#### A* Search

![image-20220429094250229](https://ik.imagekit.io/haochen/Typora/image-20220429094250229.png)

Cost + Heuristic in the Table



![image-20220430125107889](https://ik.imagekit.io/haochen/Typora/image-20220430125107889.png)



![image-20220430125058658](https://ik.imagekit.io/haochen/Typora/image-20220430125058658.png)

Go to the shortest node

Is it **Complete** and **Optimal** ?

<img src="https://ik.imagekit.io/haochen/Typora/image-20220429094731245.png" alt="image-20220429094731245" style="zoom:50%;" />

<img src="https://ik.imagekit.io/haochen/Typora/image-20220429094803408.png" alt="image-20220429094803408" style="zoom:33%;" />

<img src="https://ik.imagekit.io/haochen/Typora/image-20220429094917539.png" alt="image-20220429094917539" style="zoom:33%;" />

<img src="https://ik.imagekit.io/haochen/Typora/image-20220429095348243.png" alt="image-20220429095348243" style="zoom:33%;" />

<img src="https://ik.imagekit.io/haochen/Typora/image-20220429095432263.png" alt="image-20220429095432263" style="zoom:50%;" />

![image-20220429174329935](https://ik.imagekit.io/haochen/Typora/image-20220429174329935.png)

<img src="https://ik.imagekit.io/haochen/Typora/image-20220429095558380.png" alt="image-20220429095558380" style="zoom:50%;" />

### Optimization

<img src="https://ik.imagekit.io/haochen/Typora/image-20220429100006207.png" alt="image-20220429100006207" style="zoom:50%;" />

We use the same amount of space, don’t need to maintain the path (Tree).

Suitable for large $n$ problem.

#### Bin Packing Problem

<img src="https://ik.imagekit.io/haochen/Typora/image-20220429100258701.png" alt="image-20220429100258701" style="zoom:33%;" />

<img src="https://ik.imagekit.io/haochen/Typora/image-20220429100419645.png" alt="image-20220429100419645" style="zoom:50%;" />

### Formulating

![image-20220429101029311](https://ik.imagekit.io/haochen/Typora/image-20220429101029311.png)





<img src="https://ik.imagekit.io/haochen/Typora/image-20220429101300825.png" alt="image-20220429101300825" style="zoom:50%;" />

How to break down problem into pieces: Variables, Constraint and Objective Functions



### Hill Climbing

is a greedy local-search problem

<img src="https://ik.imagekit.io/haochen/Typora/image-20220429102206823.png" alt="image-20220429102206823" style="zoom:50%;" />

**How to generate the neighbour?**

TODO

![image-20220429102710129](https://ik.imagekit.io/haochen/Typora/image-20220429102710129.png)

### Simulated Annealing

Random Search: Now we only generate one random neighbour.

![image-20220429103752611](https://ik.imagekit.io/haochen/Typora/image-20220429103752611.png)



Until a number of iterations reached.

<img src="https://ik.imagekit.io/haochen/Typora/image-20220429103307165.png" alt="image-20220429103307165" style="zoom: 33%;" />

Probability to accept the worse quality assiciated with $Temperature$. Explore and exploit.

<img src="https://ik.imagekit.io/haochen/Typora/image-20220429103848712.png" alt="image-20220429103848712" style="zoom: 33%;" />

$\Delta E$ is the difference quality of the certer and its neighbour.

$T$ is the temperature.

<img src="https://ik.imagekit.io/haochen/Typora/image-20220429104215680.png" alt="image-20220429104215680" style="zoom: 33%;" />

**Schedule** to Reduce the T over time

<img src="https://ik.imagekit.io/haochen/Typora/image-20220429104406208.png" alt="image-20220429104406208" style="zoom:50%;" />

<img src="https://ik.imagekit.io/haochen/Typora/image-20220429104451687.png" alt="image-20220429104451687" style="zoom:33%;" />

Time for searching can be worse than Brutal.

#### TSP Problem

<img src="https://ik.imagekit.io/haochen/Typora/image-20220430131051658.png" alt="image-20220430131051658" style="zoom:50%;" />

<img src="https://ik.imagekit.io/haochen/Typora/image-20220429104802363.png" alt="image-20220429104802363" style="zoom:50%;" />

#### Contraint

<img src="https://ik.imagekit.io/haochen/Typora/image-20220429105218428.png" alt="image-20220429105218428" style="zoom:50%;" />





We need to design strategy to handle constraint

![image-20220429105826610](https://ik.imagekit.io/haochen/Typora/image-20220429105826610.png)

![image-20220429105815067](https://ik.imagekit.io/haochen/Typora/image-20220429105815067.png)

<img src="https://ik.imagekit.io/haochen/Typora/image-20220429105755676.png" alt="image-20220429105755676" style="zoom:50%;" />

### Design Objective

**Handle constraint as penalty**: Duplicated visiting or missing visiting

![image-20220527150409623](https://ik.imagekit.io/haochen/Typora/image-20220527150409623.png)

Can also be treated as quadratic function.

**Completeness:** guarentee to find solution?

<img src="https://ik.imagekit.io/haochen/Typora/image-20220527151138331.png" alt="image-20220527151138331" style="zoom:25%;" />

#### Application Example

![image-20220527154957460](https://ik.imagekit.io/haochen/Typora/image-20220527154957460.png)

![image-20220527155218830](https://ik.imagekit.io/haochen/Typora/image-20220527155218830.png)



Any optimizastion needs to be formulated first.

##### VLSI Problem

<img src="https://ik.imagekit.io/haochen/Typora/image-20220527162325864.png" alt="image-20220527162325864" style="zoom:33%;" />

<img src="https://ik.imagekit.io/haochen/Typora/image-20220527162434198.png" alt="image-20220527162434198" style="zoom:33%;" />





### ArcFace

[PDF](https://arxiv.org/pdf/1801.07698v3.pdf), [Code](https://github.com/deepinsight/insightface)

**Why not just use triplet?**



For simplicity, we fix bias $b_j=0$, and transform

$W_j^T x_i$ = $\|W_j\|\ \|x_i\|\ cos\theta_j$



where $\theta_j$ is the angle between weight $W_j$ and the feature $x_i$. 

we using $l_2$ normalization to fix $W=1$, and $x$, which then be re-scaled to $s$ (radius) makes the predictions only depend on the angle $\theta$.
$$
L=-\frac{1}{N} \sum_{i=1}^{N} \log \frac{e^{s \cos (\theta_{y_{i}}+m)}}{e^{s \cos (\theta_{y_{i}}+m)}+\sum_{j=1, j \neq y_{i}}^{n} e^{s \cos \theta_{j}}}
$$


where $m$ denotes the additive angular margin penalty between $x$ and $W_{y_i}$ to simultaneously enhance the compactness and discrepancy of intra-/inter-class.



