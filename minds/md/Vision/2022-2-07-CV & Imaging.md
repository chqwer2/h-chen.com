---
layout:     post
title:      "CV & Imaging- UoB"
subtitle:   " \"Basic Computer Vision Introduction!\""
date:       2022-02-07 12:00:00
author:     "Calvchen"
header-img: "img/post-bg-universe.jpg"
catalog: true
tags:
    - Math
    - CV
    - ML
    - AI
    - Course


---

> “Computer Vision algorithms enable machine to indentify and classify objects, then react accordingly. ”

# CV & Imaging

# Week 1 

Lecturer: Prof. Hamid Dehghani

F2F: 12 noon Weds， 3pm friday, 10am Mon-Zoom (Lab- section)



Matlab-Based tutorial

Robotic Vision

### Content 

[pdf]([Lecture 1.1 - Introduction.pdf](file:///C:/Users/calvchen/Downloads/Lecture 1.1 - Introduction.pdf))

Start with what we look up things: to know what is where, by looking. P15~16

![image-20220202122445231](https://ik.imagekit.io/haochen/Typora/image-20220202122445231.png)

prior knowledge (physics etc.) matters

![image-20220202122613036](https://ik.imagekit.io/haochen/Typora/image-20220202122613036.png)

### Evolution of Eyes

We see things cuz of light reflect..

![image-20220202122917247](https://ik.imagekit.io/haochen/Typora/image-20220202122917247.png)

different fre light react diff with material, thus how we collect the info.

Humans perceive elevtromagnetic radiation with wavelengths 360-760nm 
$$
f=\frac{c}{\lambda}\\
E=hf
$$
E is Eberfy, c = speed of light, $\lambda$ = wavalength(m), h= Plank’s constant (6.623x$10^{32}$ Js)

- **Photocell**

Only capture light from 1-direction

- Multi cell

capture different intensity of the light with better direction resolution

**Pin Hole** for only projection

It is now a sharp image but throw away lot of info

![image-20220202123803652](https://ik.imagekit.io/haochen/Typora/image-20220202123803652.png)

Lenses

**Snell’s Law** - looks more shallow than real

![image-20220202123940981](https://ik.imagekit.io/haochen/Typora/image-20220202123940981.png)

So a lens to collect and focus more info…. with the snell’s law.

pupil control the light amount





![image-20220202124102958](https://ik.imagekit.io/haochen/Typora/image-20220202124102958.png)

![image-20220202124315175](https://ik.imagekit.io/haochen/Typora/image-20220202124315175.png)

Up-side brian process the down-side vision ?????

Check it out.

What our eyes see is actually upside-down.. 





![image-20220202124551351](https://ik.imagekit.io/haochen/Typora/image-20220202124551351.png)

How much magnify or reduce the image.

![image-20220202124708640](https://ik.imagekit.io/haochen/Typora/image-20220202124708640.png)

the back of the eyes is not flat..P34



### Retina 

Contains two types of Photorecepetors

- **Rods (光杆)** ~120M, sensitive but lack of spatial resolution as they converge to the same neuron within Retina. 
- **Cones (视杆)** ~6M, active at higher light levels with higher resolution as signal processed be several neurons.

![image-20220204150317126](https://ik.imagekit.io/haochen/Typora/image-20220204150317126.png)

![image-20220204150547970](https://ik.imagekit.io/haochen/Typora/image-20220204150547970.png)

### Receptive Field

RF is the area on which light must fall for neuron to be simulated

two types of Ganglion cells: : "on-center"  and "off-center"

-  On-center: stimulated  when the center of its  receptive field is exposed  to light, and is inhibited  when the surround is  exposed to light.  
-  Off-center cells have just  the opposite reaction

[Lecture 1.2 - Human Vision (1).pdf ](file:///C:/Users/calvchen/Downloads/Lecture 1.2 - Human Vision (1).pdf) P12 ~ 13



![image-20220204152139648](https://ik.imagekit.io/haochen/Typora/image-20220204152139648.png)

some ganglion cells are sensitive with the boundry…

Need more reading of the slides…..?

The rate of firing also tells info.

![image-20220204152436769](https://ik.imagekit.io/haochen/Typora/image-20220204152436769.png)

![image-20220204152731652](https://ik.imagekit.io/haochen/Typora/image-20220204152731652.png)

No.3 Not a total crossover? but a partial crossover. Cuz the brain needs info from both sides.



- Vision generated by  photoreceptors in the retina 
- The information leaves the eye by  way of the optic nerve 
- There is a partial crossing of axons  at the optic chiasm.  
- After the chiasm, the axons are  called the optic tract.  •
- The optic tract wraps around the  midbrain to get to the lateral  geniculate nucleus (LGN) 
- The LGN axons fan out through  the deep white matter of the brain  and ultimately travel to primary  visual cortex, at the back of the  brain.



##### Where is the Color?

Three diff types of Cones.

Thrichromatic Coding…

![image-20220204153057623](https://ik.imagekit.io/haochen/Typora/image-20220204153057623.png)

Why so less blue cones?

How to discriminate wavelengths 2nm in difference?

camera has filters allow only one type of color light to go through

### Colour Mixing

**But some colors do not exist?**

One can imaging Bluish-green or  Yellowish-green, But NOT Greenish- red or Bluish-yellow!

Many forms of colour vision proposed – Until recently some hard to disapprove • 

1930s: Hering (German Physiologist)  suggested colour may be represented in visual  system as ‘opponent colours’ 

Yellow, Blue, Red and Green – Primary colours 

- Trichromatic theory cannot explain why yellow is a  primary colour

##### Opponent Process Coding 

Bluish green, yellowish green, orange (red and yellow),  purple (red and blue) OK 

- Reddish green?? Bluish Yellow?? 
  - Opposite to each other

![image-20220204153655853](https://ik.imagekit.io/haochen/Typora/image-20220204153655853.png)

Excitation and inhibition cancel each other; no change in signal.

We have Red-green Ganglion cell and Yellow-blue ganglion cell. 



# Week 2

## Edge Detection 

image scale function in matlab

squeeze -> edge more visible

Gradient of the intensity, namely how fast the pixel changing in intensity:


$$
G_x=\frac{df}{dx},\ G_y=\frac{df}{dy},\\
M(\vec G) = \sqrt{G_x^2 + G_y^2}\\
a(x,y) = \tan^{-1}\left( \frac{G_y}{G_x}\right)
$$

$$
What\ is\ \theta=\atan2(G_x, G_y)  
$$

$M$ is for Magnitude, $a$ is for direction.

### Operators or Masks

2 by 2 matrix for the conner:
$$
G_x = \begin{bmatrix}
-1 & 1 \\
-1 & 1
\end{bmatrix},\
G_y = 
\begin{bmatrix}
1 & 1 \\
-1 & -1
\end{bmatrix}
$$
![image-20220209122217135](https://chqwer2.github.io/img/Typora/image-20220209122217135.png)

**Robert**
$$
G_x \begin{bmatrix}
1 & 0 \\
0 & -1
\end{bmatrix}\ G_y \begin{bmatrix}
0 & -1\\
1 & 0
\end{bmatrix}
$$
**Sobel**
$$
G_x \begin{bmatrix}
-1 & 0 & 1 \\
-2 & 0 & 2 \\
-1 & 0 & 1
\end{bmatrix}\ G_y \begin{bmatrix}
1 & 2 & 1 \\
0 & 0 & 0 \\
-1 & -2 & -1
\end{bmatrix}
$$
Then we can get a gradient matrix, by apply threshold we can get a binary edge image.

Edge value is actually comply **Gaussian** Ditribution, but can be quite noisy.

If we set up a threshold, we may get multi-border lines. Thus the utilization of **Canny**.

#### Gaussian (Canny) edge detection 

1. Apply [Gaussian filter](https://en.wikipedia.org/wiki/Gaussian_filter) to smooth the image in order to remove the noise
2. Find the intensity gradients of the image, using [Roberts](https://en.wikipedia.org/wiki/Roberts_Cross), [Prewitt](https://en.wikipedia.org/wiki/Prewitt_operator), or [Sobel](https://en.wikipedia.org/wiki/Sobel_operator), etc.
3. Apply gradient magnitude thresholding or lower bound cut-off suppression to get rid of spurious response to edge detection
4. Apply double threshold to determine potential edges
5. Track edge by [hysteresis](https://en.wikipedia.org/wiki/Hysteresis): Finalize the detection of edges by suppressing all the other edges that are weak and not connected to strong edges.



## Filtering

Highly Directed Work

- Second order operators 
- Thresholding



Mean filter: 

random distributed noisy (even out positive and negative noise)

Gaussian Filter:

![image-20220211153730108](https://ik.imagekit.io/haochen/Typora/image-20220211153730108.png)
$$
G_{2D} = G_{1D}*G_{1D}^T
$$


### Laplacian Operator

![image-20220211154327251](https://ik.imagekit.io/haochen/Typora/image-20220211154327251.png)

It is good to have Second Derivative, zero crossing points can be a good edge estimator, but not robust for noise.
$$
I\otimes G_{2d}\otimes\mathcal L
$$



So $ G_{2d}\otimes\mathcal L$ can be a new filter called LoG
$$
\nabla \partial
$$

### Advanced Edge Detection

What cause intensity changes?

- Gemetric: 

  surface orientation, depth, color and texture discontinuities

- Non-geometric:

  illumination, specularities (镜面反射), shadows and inter-reflections.

![image-20220216121320738](https://chqwer2.github.io/img/Typora/image-20220216121320738.png)

**Edge Descriptors**

Direction - perpendicular to the direction  of maximum intensity change (i.e., edge normal)

Strength - related to  the local image contrast along  the normal

And Position

<img src="https://chqwer2.github.io/img/Typora/image-20220216121520636.png" alt="image-20220216121520636" style="zoom:50%;" />

**Main Step in ED**

(1) Smoothing: suppress as much noise as possible,  without destroying true edges. 

(2) Enhancement: apply differentiation to enhance the  quality of edges (i.e., sharpening)

(3) Thresholding: determine which edge pixels  should be discarded as noise and which should be  retained (i.e., threshold edge magnitude).

(4) Localization: determine the exact edge location. 

Upsample: sub-pixel resolution might be required for some applications to  estimate the location of an edge to better than the spacing  between pixels

![image-20220216121830295](https://chqwer2.github.io/img/Typora/image-20220216121830295.png)

But it is super noise..

![image-20220216121941953](https://chqwer2.github.io/img/Typora/image-20220216121941953.png)



h is a Gaussian filter, but sliterly blur my edge

![image-20220216122021305](https://chqwer2.github.io/img/Typora/image-20220216122021305.png)

instead conv of h and f, we can also take differentiated G which saves one operation 

![image-20220216122114369](https://chqwer2.github.io/img/Typora/image-20220216122114369.png)

### Prewitt Operator

$$
G_x \begin{bmatrix}
-1 & 0 & 1 \\
-1 & 0 & 1 \\
-1 & 0 & 1
\end{bmatrix}\ G_y \begin{bmatrix}
-1 & -1 & -1 \\
0 & 0 & 0 \\
1 & 1 & 1
\end{bmatrix}
$$

![image-20220216122423103](https://chqwer2.github.io/img/Typora/image-20220216122423103.png)

##### Practical Issue

Noise suppression-localization tradeoff. 

– Smoothing depends on mask size (e.g., depends on σ for  Gaussian filters). 

– Larger mask sizes reduce noise, but worsen localization (i.e.,  add uncertainty to the location of the edge) and vice versa

![image-20220216122521720](https://chqwer2.github.io/img/Typora/image-20220216122521720.png)

We want good localzation and single response.

### Canny Edge Detector

![image-20220216123128065](https://chqwer2.github.io/img/Typora/image-20220216123128065.png)

![image-20220216123149604](https://chqwer2.github.io/img/Typora/image-20220216123149604.png)

<img src="https://chqwer2.github.io/img/Typora/image-20220216123252181.png" alt="image-20220216123252181" style="zoom:50%;" />

<img src="https://chqwer2.github.io/img/Typora/image-20220216123442349.png" alt="image-20220216123442349" style="zoom:44%;" />

I got a thick edge, but not I chose the local maximum of the edge gradient direction.

**Non-maxima suppression**

Check if gradient magnitude at pixel location (i,j) is local maximum along gradient direction



### Hysteresis thresholding

Standard thresholding can only select “strong” edges, does not guarantee “continuity”.

![image-20220216123859997](https://chqwer2.github.io/img/Typora/image-20220216123859997.png)

![image-20220216124005724](https://chqwer2.github.io/img/Typora/image-20220216124005724.png)



### Scale Invariant Feature Transform (SIFT)

Given the noisy image, design the best suitable  algorithm to detect  edges.

Given the calculated edges, how would you quantify accuracy?

**Why we want to match features？**

Tasks like Object Recognition, Tracking…

- Good features should be robust to all sorts of  nastiness that can occur between images.

Types of invariance:

- illumination
- Scale
- Rotation
- Affine
- Full Perspective 

<img src="https://chqwer2.github.io/img/Typora/image-20220303142343127.png" alt="image-20220303142343127" style="zoom:23%;" /><img src="https://chqwer2.github.io/img/Typora/image-20220303142237212.png" alt="image-20220303142237212" style="zoom:33%;" /><img src="https://chqwer2.github.io/img/Typora/image-20220303142250380.png" alt="image-20220303142250380" style="zoom:28%;" />

How to achieve illumination invariance?

- The easy way (normalized) - histogram
- Difference based metrics (sift)

How to achieve scale invariance?

- Pyramids
  - Down Sampling
  - Repeat until image is tiny
  - Run filter over each size image and hope its robust

<img src="https://chqwer2.github.io/img/Typora/image-20220303143403986.png" alt="image-20220303143403986" style="zoom:50%;" />

- Scale Space (**Different Of Gaussian** (DOG) method) ?# Todo
  - Pyramid but fill gaps with blurred images
  - not down sampling, but blurring it..
  -  Like having a nice linear scaling without the  expense
  - Take features from differences of these images
  - If the feature is repeatably present in between  Difference of Gaussians it is Scale Invariant and  we should keep it.

<img src="https://chqwer2.github.io/img/Typora/image-20220303143703743.png" alt="image-20220303143703743" style="zoom:50%;" />

### Rotation Invariance

- Rotate all features to go the same way in a  determined manner 
- Take histogram of Gradient directions  
- Rotate to most dominant (maybe second if its  good enough

If rotation, looking at the **histogram**: will be same distribution but offset.



### Handout 4.2 

Hough Transform

Polar Space and Cartesian Space

coordinaties

Distance from the origin

<img src="https://chqwer2.github.io/img/Typora/image-20220225151010677.png" alt="image-20220225151010677" style="zoom:33%;" />

![image-20220225151137606](https://chqwer2.github.io/img/Typora/image-20220225151137606.png)



The Hough transform is a common  approach to finding  parameterised line segments  (here straight lines

**The basic idea:**

Each straight line in image can be described by an equation ($\mathbf w, \phi$), $\phi$ for the angle.

Each isolated point can lie on an infinite number of straight lines.

In the Hough transform each point votes for  every line it could be on. 

The lines with the most votes win.

**Hough Space **

($\mathbf w, \phi$)

![image-20220225152551399](https://chqwer2.github.io/img/Typora/image-20220225152551399.png)

It also conduct NMS to gain the best edge.

We need to set a threshold $A$, which is the minPoint to create a line.

A hough map

![image-20220225153059790](https://chqwer2.github.io/img/Typora/image-20220225153059790.png)

There are generalised versions for ellipses, circles 

For the straight line transform we need to supress non-local maxima 

The input image could also benefit from edge thinning 

Single line segments not isolated 

Will still fail in the face of certain textures



### Circle Hough Transform

![image-20220225153622056](https://chqwer2.github.io/img/Typora/image-20220225153622056.png)

Hough transform technique is that it is **tolerant of gaps in feature boundary descriptions and is relatively unaﬀected by image noise**, unlike edge detecto

### Lecture 5. Image Registration

Segmentation of Ageing brain

<img src="C:\Users\calvchen\PycharmProjects\chqwer2.github.io\_posts\UoB-Ongoing\image-20220302120932875.png" alt="image-20220302120932875" style="zoom:33%;" />

atlas 地图集



- Geometric (and Photometric) alignment of  one image with another 

- Implemented as the process of estimating an  optimal transformation between two images.

- Images may be of same or different types  (MR, CT, visible, fluorescence, ...)

  

Co-register the image

![image-20220302121040053](C:\Users\calvchen\PycharmProjects\chqwer2.github.io\_posts\UoB-Ongoing\image-20220302121040053.png)



![image-20220302121317303](C:\Users\calvchen\PycharmProjects\chqwer2.github.io\_posts\UoB-Ongoing\image-20220302121317303.png)

![image-20220302121454461](C:\Users\calvchen\PycharmProjects\chqwer2.github.io\_posts\UoB-Ongoing\image-20220302121454461.png)

![image-20220302121739243](C:\Users\calvchen\PycharmProjects\chqwer2.github.io\_posts\UoB-Ongoing\image-20220302121739243.png)

Landmarks: eyes, ears etc. or curve of features

Image values: conservation of intensity

![image-20220302122100956](C:\Users\calvchen\PycharmProjects\chqwer2.github.io\_posts\UoB-Ongoing\image-20220302122100956.png)

need same dimension of resolution

hard to handle different features

![image-20220302122258805](https://chqwer2.github.io/img/Typora/image-20220302122258805.png)

different pixels value are more likely to belong to different group.

**The joint histogram**

![image-20220302122501565](https://chqwer2.github.io/img/Typora/image-20220302122501565.png)

![image-20220302122544856](https://chqwer2.github.io/img/Typora/image-20220302122544856.png)

Class of Transforms:

- Rigid, not scaling (6-dimension)

  ![image-20220302122920286](https://chqwer2.github.io/img/Typora/image-20220302122920286.png)

  ![image-20220302122931772](https://chqwer2.github.io/img/Typora/image-20220302122931772.png)

- Affine

  ![image-20220302123024721](https://chqwer2.github.io/img/Typora/image-20220302123024721.png)

- Piecewise Affine

  Typically use different affine transformation for  different parts of the image

  <img src="https://chqwer2.github.io/img/Typora/image-20220302123139995.png" alt="image-20220302123139995" style="zoom:50%;" />

- Non-rigid (Elastic)

  some shrinking, some expanding or deforming

  External forces drive transformation, Internal forces provide constraints.

  ![image-20220302123354950](https://chqwer2.github.io/img/Typora/image-20220302123354950.png)

**What similarity criterion to use?**

![image-20220302123531184](https://chqwer2.github.io/img/Typora/image-20220302123531184.png)

maintain the distances between features.

![image-20220302123611968](https://chqwer2.github.io/img/Typora/image-20220302123611968.png)

1. RMS

2. Mutual Info

   ![image-20220302123714703](https://chqwer2.github.io/img/Typora/image-20220302123714703.png)

   maximize the possibility of the location given the pixel.

   what is $p_{i,j}$ ?

   <img src="https://chqwer2.github.io/img/Typora/image-20220302123743633.png" alt="image-20220302123743633" style="zoom:50%;" />

3. What is **Normalised  cross-correlation**?



### Computational Vision

- Spatial resolution: Pixel Size
- Intensity resolution: Bits per pixel
- Time resolution: Frames per sec.
- **Spectral** resolution: Number of bands + bandwidth



### Characterising images as signals

**Image Statistics**

- Mean, standard deviation
- Histogram: **frequency distribution** graph

Signal-to-noise (SNR)

![image-20220304151507565](https://chqwer2.github.io/img/Typora/image-20220304151507565.png)

![image-20220304151808754](https://chqwer2.github.io/img/Typora/image-20220304151808754.png)

Non-automated: taking 5~6 and average through.

**Histogram-based segmentation**

![image-20220304150556638](https://chqwer2.github.io/img/Typora/image-20220304150556638.png)

Thresholding challenges

![image-20220304152402554](https://chqwer2.github.io/img/Typora/image-20220304152402554.png)

How do we determine the threshold ? 

Different regions / image areas may need  different levels of threshold.

Many approaches possible 

- Interactive threshold 
- Adaptive threshold 
- Variance minimisation method (Otsu threshold  selection algorithm)

**What is the OTSU?** #TODO



### Mathematical Morphology

![image-20220304152821808](https://chqwer2.github.io/img/Typora/image-20220304152821808.png)

![image-20220304152834298](https://chqwer2.github.io/img/Typora/image-20220304152834298.png)

**Dilation**

- adding a “layer” of pixels to the periphery of object

**Erosion**

- removing a “layer” of pixels all round an object

### Two advanced segmentation methods

- Active contours (snakes) 
- Watershed 
- Level-set methods  # TODO
- Active shape model segmentation # TODO



**Active contours (snakes)** 

![image-20220304153144897](https://chqwer2.github.io/img/Typora/image-20220304153144897.png)

![image-20220304153154094](https://chqwer2.github.io/img/Typora/image-20220304153154094.png)

<img src="https://chqwer2.github.io/img/Typora/image-20220304153213040.png" alt="image-20220304153213040" style="zoom:33%;" />

**Watershed Segmentation**

![image-20220304153501783](https://chqwer2.github.io/img/Typora/image-20220304153501783.png)

![image-20220304153513547](https://chqwer2.github.io/img/Typora/image-20220304153513547.png)

<img src="https://chqwer2.github.io/img/Typora/image-20220304153849584.png" alt="image-20220304153849584" style="zoom:33%;" />

### (Active) 3D Imaging and 3D

About touching the world…

<img src="https://chqwer2.github.io/img/Typora/image-20220309120343092.png" alt="image-20220309120343092" style="zoom:33%;" />

Robotic Manipulation

https://www.cs.bham.ac.uk/research/groupings/robotics

![image-20220309121543037](https://chqwer2.github.io/img/Typora/image-20220309121543037.png)

### 3D Imaging

It is hard for people to interpret the first image.

We can use both of them at the same time.

![image-20220309121832515](https://chqwer2.github.io/img/Typora/image-20220309121832515.png)

Depth versus distance

<img src="https://chqwer2.github.io/img/Typora/image-20220309121928432.png" alt="image-20220309121928432" style="zoom:33%;" />

How to measure depth and distance?

<img src="https://chqwer2.github.io/img/Typora/image-20220309122022691.png" alt="image-20220309122022691" style="zoom:50%;" />

**Passive**

- Stereophotogrammetry
- Structure from motion
- Dapth from focus

**Active**

- TOF
- Structured light imaging
- Photometric stereo

#### Stereophotogrammetry

But hard to process related image (i.e. find the matching pixels)

<img src="https://chqwer2.github.io/img/Typora/image-20220309122405711.png" alt="image-20220309122405711" style="zoom:33%;" />



<img src="https://chqwer2.github.io/img/Typora/image-20220309122300870.png" alt="image-20220309122300870" style="zoom: 50%;" />

![image-20220309122432980](https://chqwer2.github.io/img/Typora/image-20220309122432980.png)

#### Structure from motion

we have one camera, but moving…

![image-20220309122630153](https://chqwer2.github.io/img/Typora/image-20220309122630153.png)

![image-20220309123452117](https://chqwer2.github.io/img/Typora/image-20220309123452117.png)

predict where the canvas is, and needs more prior knowledge like the location of the camera.



#### Depth from focus

move the lens that focus..

looking for sharp edges, but not any time that emerges.

![image-20220309122845623](https://chqwer2.github.io/img/Typora/image-20220309122845623.png)

It is possible but it is quite noisy.



### Passive

![image-20220309123813432](https://chqwer2.github.io/img/Typora/image-20220309123813432.png)



**Active Stereophotogrammetry**

R200 Camera

- Can project surface features
- Multiple camera still do not interfere with each other

Holes if you don’t find correspondence.

![image-20220309124338528](https://chqwer2.github.io/img/Typora/image-20220309124338528.png)

### TOF

noisy when multiple objects, so we only look at one direction at once.

![image-20220309124701611](https://chqwer2.github.io/img/Typora/image-20220309124701611.png)

We now have a wave, so a wave bouncing back..

collect different pixel at different time..

![image-20220309125313247](https://chqwer2.github.io/img/Typora/image-20220309125313247.png)





Dmitry..

### Structured Light

![image-20220312171222797](https://chqwer2.github.io/img/Typora/image-20220312171222797.png)

![image-20220312171458132](https://chqwer2.github.io/img/Typora/image-20220312171458132.png)

![image-20220312173308429](https://chqwer2.github.io/img/Typora/image-20220312173308429.png)

![image-20220312173934967](https://chqwer2.github.io/img/Typora/image-20220312173934967.png)

Phase wrapping and unwrapping

![image-20220312174401909](https://chqwer2.github.io/img/Typora/image-20220312174401909.png)

![image-20220312174427902](https://chqwer2.github.io/img/Typora/image-20220312174427902.png)

### Photometric stereo

goal is not the depth, but the surfaces…

<img src="https://chqwer2.github.io/img/Typora/image-20220312174813723.png" alt="image-20220312174813723" style="zoom:80%;" />

![image-20220312174948739](https://chqwer2.github.io/img/Typora/image-20220312174948739.png)

![image-20220312175026363](https://chqwer2.github.io/img/Typora/image-20220312175026363.png)

![image-20220312175131814](https://chqwer2.github.io/img/Typora/image-20220312175131814.png)

### 3D Structure Data

convert depth data into point cloud

![image-20220312175228990](https://chqwer2.github.io/img/Typora/image-20220312175228990.png)

![image-20220312175236959](https://chqwer2.github.io/img/Typora/image-20220312175236959.png)

![image-20220312175248254](https://chqwer2.github.io/img/Typora/image-20220312175248254.png)

Try to find the function to build surfaces (gradient)

![image-20220312175347447](https://chqwer2.github.io/img/Typora/image-20220312175347447.png)

Representations: Untextured mesh and textured mesh

![image-20220312175510489](https://chqwer2.github.io/img/Typora/image-20220312175510489.png)

![image-20220312175521335](https://chqwer2.github.io/img/Typora/image-20220312175521335.png)

### Collecting multiple views of a scene (world  coordinates)

Robot coordinates

<img src="https://chqwer2.github.io/img/Typora/image-20220312175607060.png" alt="image-20220312175607060" style="zoom:33%;" />

![image-20220312175636834](https://chqwer2.github.io/img/Typora/image-20220312175636834.png)

### How to combine point cloud?

![image-20220312175750995](https://chqwer2.github.io/img/Typora/image-20220312175750995.png)

![image-20220312175807276](https://chqwer2.github.io/img/Typora/image-20220312175807276.png)

![image-20220312175823028](https://chqwer2.github.io/img/Typora/image-20220312175823028.png)

<img src="https://chqwer2.github.io/img/Typora/image-20220312175831685.png" alt="image-20220312175831685" style="zoom:50%;" />

![image-20220312175850854](https://chqwer2.github.io/img/Typora/image-20220312175850854.png)

![image-20220312175859645](https://chqwer2.github.io/img/Typora/image-20220312175859645.png)

ICP algorithm

![image-20220312175930891](https://chqwer2.github.io/img/Typora/image-20220312175930891.png)

![image-20220312175939080](https://chqwer2.github.io/img/Typora/image-20220312175939080.png)

Multi-steps…

<img src="https://chqwer2.github.io/img/Typora/image-20220312180016880.png" alt="image-20220312180016880" style="zoom:25%;" /><img src="https://chqwer2.github.io/img/Typora/image-20220312180030879.png" alt="image-20220312180030879" style="zoom:25%;" /><img src="https://chqwer2.github.io/img/Typora/image-20220312180041720.png" alt="image-20220312180041720" style="zoom:25%;" /><img src="https://chqwer2.github.io/img/Typora/image-20220312180055579.png" alt="image-20220312180055579" style="zoom:25%;" /><img src="https://chqwer2.github.io/img/Typora/image-20220312180103211.png" alt="image-20220312180103211" style="zoom:25%;" />

![image-20220312180120023](https://chqwer2.github.io/img/Typora/image-20220312180120023.png)

ICP…



Others

![image-20220312180143578](https://chqwer2.github.io/img/Typora/image-20220312180143578.png)



### Principal Components Analysis (PCA)

**Covariance** 

measure of how much each of the dimensions vary from the mean with  respect to each other

Covariance Matrix

![image-20220316120808560](https://chqwer2.github.io/img/Typora/image-20220316120808560.png)

- Diagonal is the variances of x, y and z 

- $cov(x,y) = cov(y,x)$ hence matrix is symmetrical about the  diagonal 
- N-dimensional data will result in NxN covariance matrix



**How to interpret covariance?**

The value itself that it doesn’t mean anything, but can use to determine the correlation and its **sign**.

If it is 0: they are independent.

### PCA

It can simplify a dataset $\in R^d$

- A linear transformation that chooses a new coordinate system for  the data set such that: 
- greatest variance by any projection of the data set comes to lie on  the first axis (then called the first principal component), 
- the second greatest variance on the second axis, 
- and so on 

It eliminates the later components for reducing dimensianlity.

The dimensions in PCA will be orthonal.





What is the principal component. 

- By finding the **eigenvalues and eigenvectors** of the covariance  matrix, we find that the eigenvectors with the **largest eigenvalues**  correspond to the dimensions that have the **strongest correlation** in  the dataset. 

PCA is a useful statistical technique that has found  application in: 

- fields such as face recognition and image compression 
- finding patterns in data of high dimension

### Basic Theory

![image-20220316122431185](https://chqwer2.github.io/img/Typora/image-20220316122431185.png)

Then, we gain the covariance matrix:

![image-20220316122548399](https://chqwer2.github.io/img/Typora/image-20220316122548399.png)

N can be the number of pixels in an image.

![image-20220316122939584](https://chqwer2.github.io/img/Typora/image-20220316122939584.png)

How much that features contribute, and choose the top-k features.

![image-20220316123256574](https://chqwer2.github.io/img/Typora/image-20220316123256574.png)



##### **Example**

![image-20220316123601511](https://chqwer2.github.io/img/Typora/image-20220316123601511.png)

![image-20220316123616267](https://chqwer2.github.io/img/Typora/image-20220316123616267.png)

![image-20220316123637101](https://chqwer2.github.io/img/Typora/image-20220316123637101.png)

![image-20220316123738019](https://chqwer2.github.io/img/Typora/image-20220316123738019.png)

![image-20220316123748143](https://chqwer2.github.io/img/Typora/image-20220316123748143.png)



### Singular Value Decomposition (SVD)

![image-20220316124228097](https://chqwer2.github.io/img/Typora/image-20220316124228097.png)

![image-20220316124238188](https://chqwer2.github.io/img/Typora/image-20220316124238188.png)

### Face Recognition (Not Detection)

**Ideas:**

- Eigenfaces: the idea 
- Eigenvectors and Eigenvalues 
- Co-variance 
- Learning Eigenfaces from training sets of faces 
- Recognition and reconstruction

**Eigenfaces**

Think face as a combination of some **components** of faces.

These basis faces can be differently weighted to represent any faces

So we can use different vectors of weights to represent faces.

![image-20220318150856393](https://chqwer2.github.io/img/Typora/image-20220318150856393.png)

**How do we pick the set of basis faces?**

**Statistical criterion** for measuring the notion of “best representation of the differences between the training faces”

**How to learn?**

<img src="https://chqwer2.github.io/img/Typora/image-20220318151322938.png" alt="image-20220318151322938" style="zoom:50%;" />

1. training set rearrange into **2Dmatrix**…
   - Rows: Each value, Columns: Each pixel value
2. Calculate Co-variance matrix
3. Then find the **eigenvectors** of that covariance matrix.
4. Sort by **eigenvalues** and find the top-features.
5. Get the principal components $v_k$

**Image space to face space.**

![image-20220318151950175](https://chqwer2.github.io/img/Typora/image-20220318151950175.png)

**Recognition in face space**

![image-20220318152127939](https://chqwer2.github.io/img/Typora/image-20220318152127939.png)

The cloest face in the face space is the chosen match.

But if with hat or glasses ???

**Image registration**

Some Books

<img src="https://ik.imagekit.io/haochen/Typora/image-20220427121459236.png" alt="image-20220427121459236" style="zoom:33%;" />



Image Matting Problem

<img src="https://ik.imagekit.io/haochen/Typora/image-20220427122141989.png" alt="image-20220427122141989" style="zoom:50%;" />



![image-20220429150059601](https://ik.imagekit.io/haochen/Typora/image-20220429150059601.png)

<img src="https://ik.imagekit.io/haochen/Typora/image-20220429150532191.png" alt="image-20220429150532191" style="zoom:50%;" />

<img src="https://ik.imagekit.io/haochen/Typora/image-20220429150614080.png" alt="image-20220429150614080" style="zoom:50%;" />



![image-20220429151321024](https://ik.imagekit.io/haochen/Typora/image-20220429151321024.png)



<img src="https://ik.imagekit.io/haochen/Typora/image-20220429152721881.png" alt="image-20220429152721881" style="zoom:50%;" />

Open Source **Detectron2** based on PyTorch

<img src="https://ik.imagekit.io/haochen/Typora/image-20220429154132544.png" alt="image-20220429154132544" style="zoom:50%;" />

Temporal superresolution

![](https://ik.imagekit.io/haochen/Typora/image-20220429154239642.png)

![image-20220429154342730](https://ik.imagekit.io/haochen/Typora/image-20220429154342730.png)

![image-20220429154441083](https://ik.imagekit.io/haochen/Typora/image-20220429154441083.png)

![image-20220429154600358](https://ik.imagekit.io/haochen/Typora/image-20220429154600358.png)

 

![image-20220429154837326](https://ik.imagekit.io/haochen/Typora/image-20220429154837326.png)

![image-20220429154953257](https://ik.imagekit.io/haochen/Typora/image-20220429154953257.png)

![image-20220429155103422](https://ik.imagekit.io/haochen/Typora/image-20220429155103422.png)

<img src="https://ik.imagekit.io/haochen/Typora/image-20220429155427677.png" alt="image-20220429155427677" style="zoom:50%;" />

![image-20220429155549667](https://ik.imagekit.io/haochen/Typora/image-20220429155549667.png)

![image-20220504123501347](https://ik.imagekit.io/haochen/Typora/image-20220504123501347.png)





![image-20220504124412562](https://ik.imagekit.io/haochen/Typora/image-20220504124412562.png)

![image-20220504124551446](https://ik.imagekit.io/haochen/Typora/image-20220504124551446.png)







### Initialisation:

![image-20220506152639141](https://ik.imagekit.io/haochen/Typora/image-20220506152639141.png)







